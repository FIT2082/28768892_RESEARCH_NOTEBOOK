# Week 9
## Meeting
This week’s meeting was mainly demonstrating the progress we have made with the project and the deliverable we have developed so far. We agreed to send screenshot and videos as we progress during the coming weeks. Since our aim is to have everything working in VR first, we also decided it was a good time to transition everything we have in our Unity environment to work in VR as this will help us better test the things we implement. 
## Before next meeting
This week we decided to split the development work into two different sections. One was working on the input data stream for the graph and manipulating the graphs using the graph maker asset, while the second part was working on making those graphs appear in the correct position when a gesture is performed. The second part was what I focused on, I worked on creating gestures using the leap motion libraries. There was a lot of trial and error with creating these gestures. The main gesture was the pointing finger. The objective of the pointing finger was to be used to select objects that we will eventually want to perform some query on. This problem was broken up into another two sub problems, the first sub problem was detecting that the leap hands are making the pointing gesture. Once we were able to detect this, then we want to get the direction that the finger is pointing and return the first object in its path, but this object cannot just be any object it has to be an object that we have data for and are able to perform some query on. In order to achieve the desired outcome for the second sub problem, we first need to shoot a Raycast(virtual laser) in the direction that the index finger is pointing and return the first object that it hit but this hit is filtered that that it only detects a hit if it the type of object that we want (machinery). Finally, once we have successfully performed these, we can set the selected object to the object that was returned from the hit. We make this object glow so that the user is aware that the object is selected.  Another gesture we had to work on was one that would be used by the user to display the data for the selected object. To show the graph we used a gesture of opening hands while palms are facing up and facing them down to remove the graph. There were other ideas for removing the graph such swiping away, but we were unable to implement these as the new version of leap motion had removed functionally to support these types of gestures. The graphs are displayed in front of the object that is selected. Getting the positioning of the graph was a little tricky and took a lot of time to get correct because we wanted to make it as natural as possible where it didn’t just pop up anywhere and look out of place, we tried our best to make the graphs not affect the environment around it by covering anything important or not being clear what object it belonged to. 
